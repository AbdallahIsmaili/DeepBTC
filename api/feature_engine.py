# api/feature_engine.py - OPTIMIZED VERSION (95%+ data retention)

import pandas as pd
import pandas_ta as ta
import os
import numpy as np
from datetime import datetime
import warnings

warnings.filterwarnings('ignore', category=FutureWarning)

# --- Configuration ---
RAW_DATA_PATHS = {
    'ohlcv': 'data/raw/binance_btcusdt_1h.csv',
    'blockchain': 'data/raw/blockchain_metrics_daily.csv',
    'sentiment': 'data/raw/sentiment_metrics.csv',
    'macro': 'data/raw/macro_indicators.csv'
}

FEATURE_DATA_PATH = 'data/features/btc_features_complete.csv'
OUTPUT_DIR = 'data/features'

class EnhancedFeatureEngine:
    """
    OPTIMIZED: Maximizes data retention while combining multiple sources.
    
    Key improvements:
    1. Smarter forward-filling strategy for external data
    2. Only drops rows where CRITICAL features are missing
    3. Handles date misalignment gracefully
    4. Expected retention: 90-95% (vs previous 32%)
    """
    
    def __init__(self):
        self.data = {}
        self.critical_features = ['Close', 'Volume', 'returns']  # Must have these
    
    def load_all_data(self):
        """Loads all available data sources with robust error handling."""
        
        print("Loading all data sources...")
        print("="*50)
        
        # Load OHLCV (required)
        if not os.path.exists(RAW_DATA_PATHS['ohlcv']):
            print(f"‚ùå ERROR: OHLCV data not found at {RAW_DATA_PATHS['ohlcv']}")
            print("Please run: python main.py fetch")
            return False
        
        self.data['ohlcv'] = pd.read_csv(
            RAW_DATA_PATHS['ohlcv'], 
            index_col='Datetime', 
            parse_dates=True
        )
        print(f"‚úÖ OHLCV loaded: {self.data['ohlcv'].shape}")
        print(f"   Date range: {self.data['ohlcv'].index.min()} to {self.data['ohlcv'].index.max()}")
        
        # Load optional sources
        optional_sources = ['blockchain', 'sentiment', 'macro']
        
        for source in optional_sources:
            path = RAW_DATA_PATHS[source]
            if not os.path.exists(path):
                print(f"‚ö†Ô∏è  {source.capitalize()} data not found - will be skipped")
                continue
            
            try:
                if source == 'macro':
                    # Special handling for macro data
                    df = pd.read_csv(path)
                    
                    # Find and set date index
                    date_col = None
                    for col in ['Date', 'date', 'Datetime', 'datetime']:
                        if col in df.columns:
                            date_col = col
                            break
                    
                    if date_col:
                        df[date_col] = pd.to_datetime(df[date_col])
                        df.set_index(date_col, inplace=True)
                    else:
                        # Assume first column is date
                        df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0])
                        df.set_index(df.columns[0], inplace=True)
                    
                    self.data[source] = df
                else:
                    # Standard loading for blockchain and sentiment
                    self.data[source] = pd.read_csv(
                        path, 
                        index_col=0, 
                        parse_dates=True
                    )
                
                print(f"‚úÖ {source.capitalize()} loaded: {self.data[source].shape}")
                print(f"   Date range: {self.data[source].index.min()} to {self.data[source].index.max()}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Error loading {source}: {e}")
        
        print("="*50)
        return True
    
    def generate_price_features(self, df):
        """Generates technical indicators from OHLCV data."""
        
        print("\n[1/4] Generating price-based features...")
        initial_cols = df.shape[1]
        
        # Returns (critical features)
        df['returns'] = df['Close'].pct_change()
        df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))
        df['future_return_1h'] = df['Close'].shift(-1) / df['Close'] - 1
        df['future_return_6h'] = df['Close'].shift(-6) / df['Close'] - 1
        df['future_return_24h'] = df['Close'].shift(-24) / df['Close'] - 1
        
        # Price momentum
        df['price_momentum_24h'] = df['Close'] / df['Close'].shift(24) - 1
        df['price_momentum_7d'] = df['Close'] / df['Close'].shift(24*7) - 1
        
        # Volatility
        df['volatility_24h'] = df['returns'].rolling(24).std()
        df['volatility_7d'] = df['returns'].rolling(24*7).std()
        
        # Volume features
        df['volume_ma_24h'] = df['Volume'].rolling(24).mean()
        df['volume_ratio'] = df['Volume'] / df['volume_ma_24h']
        df['volume_momentum'] = df['Volume'] / df['Volume'].shift(24) - 1
        
        # Technical Indicators using pandas_ta
        df.ta.vwap(append=True)
        df.ta.sma(length=20, append=True)
        df.ta.sma(length=50, append=True)
        df.ta.sma(length=200, append=True)
        df.ta.ema(length=12, append=True)
        df.ta.ema(length=26, append=True)
        df.ta.macd(append=True)
        df.ta.rsi(length=14, append=True)
        df.ta.rsi(length=21, append=True)
        df.ta.stoch(append=True)
        df.ta.atr(length=14, append=True)
        df.ta.bbands(length=20, append=True)
        df.ta.adx(length=14, append=True)
        df.ta.cci(length=20, append=True)
        df.ta.willr(length=14, append=True)
        df.ta.mfi(length=14, append=True)
        df.ta.obv(append=True)
        
        # Price position relative to MAs
        df['price_to_sma20'] = df['Close'] / df['SMA_20'] - 1
        df['price_to_sma50'] = df['Close'] / df['SMA_50'] - 1
        df['price_to_sma200'] = df['Close'] / df['SMA_200'] - 1
        df['sma50_to_sma200'] = df['SMA_50'] / df['SMA_200'] - 1
        
        new_features = df.shape[1] - initial_cols
        print(f"   Generated {new_features} price features")
        return df
    
    def merge_external_data(self, df, source_name, source_df):
        """
        Generic method to merge external data with smart forward-filling.
        
        Strategy:
        1. Resample external data to hourly frequency
        2. Forward fill to propagate values (external data is daily/slower)
        3. Use LEFT join to keep all OHLCV rows
        4. Fill remaining NaNs with forward fill + backward fill
        """
        
        if source_df is None or source_df.empty:
            return df
        
        print(f"\n[{source_name}] Merging external data...")
        initial_cols = df.shape[1]
        
        # Resample to hourly and forward fill
        source_hourly = source_df.resample('1H').ffill()
        
        # Get column names before merge
        source_cols = list(source_hourly.columns)
        
        # Merge with LEFT join (keeps all OHLCV rows)
        df = df.join(source_hourly, how='left')
        
        # Smart filling strategy for merged columns
        # 1. Forward fill (propagate last known value)
        df[source_cols] = df[source_cols].ffill()
        
        # 2. Backward fill for any remaining NaNs at the start
        df[source_cols] = df[source_cols].bfill()
        
        # 3. For any still-remaining NaNs, fill with column median
        for col in source_cols:
            if df[col].isnull().any():
                median_val = df[col].median()
                df[col].fillna(median_val, inplace=True)
                print(f"   ‚ö†Ô∏è  Filled {col} NaNs with median: {median_val:.2f}")
        
        new_features = df.shape[1] - initial_cols
        print(f"   Merged {new_features} features from {source_name}")
        
        return df
    
    def add_derived_features(self, df):
        """Adds derived features from merged data."""
        
        print("\n[Derived Features] Creating cross-source features...")
        initial_cols = df.shape[1]
        
        # Blockchain-derived features
        if 'hash_rate_th_s' in df.columns:
            df['hash_rate_change_7d'] = df['hash_rate_th_s'] / df['hash_rate_th_s'].shift(24*7) - 1
            df['hash_rate_change_30d'] = df['hash_rate_th_s'] / df['hash_rate_th_s'].shift(24*30) - 1
        
        if 'difficulty' in df.columns:
            df['difficulty_change_7d'] = df['difficulty'] / df['difficulty'].shift(24*7) - 1
        
        if 'tx_count_daily' in df.columns:
            df['tx_count_change_7d'] = df['tx_count_daily'] / df['tx_count_daily'].shift(24*7) - 1
            df['tx_count_ma_7d'] = df['tx_count_daily'].rolling(24*7).mean()
        
        # NVT Ratio (if data available)
        if all(col in df.columns for col in ['market_price_usd', 'tx_count_daily', 'total_btc_supply']):
            market_cap = df['market_price_usd'] * df['total_btc_supply']
            df['nvt_ratio'] = market_cap / (df['tx_count_daily'] * 1e6)
            # Replace infinite values properly (pandas best practice)
            df['nvt_ratio'] = df['nvt_ratio'].replace([np.inf, -np.inf], np.nan)
            df['nvt_ratio'] = df['nvt_ratio'].ffill().bfill()
        
        # Sentiment-derived features
        if 'fear_greed_value' in df.columns:
            df['fear_greed_change_7d'] = df['fear_greed_value'] - df['fear_greed_value'].shift(24*7)
            df['fear_greed_ma_7d'] = df['fear_greed_value'].rolling(24*7).mean()
            df['extreme_fear'] = (df['fear_greed_value'] < 25).astype(int)
            df['extreme_greed'] = (df['fear_greed_value'] > 75).astype(int)
        
        # Macro-derived features
        if 'SP500' in df.columns:
            df['sp500_returns'] = df['SP500'].pct_change()
            df['sp500_change_7d'] = df['SP500'] / df['SP500'].shift(24*7) - 1
            # Correlation calculation
            df['btc_sp500_correlation'] = df['returns'].rolling(24*30).corr(df['sp500_returns'])
        
        if 'VIX' in df.columns:
            df['vix_change'] = df['VIX'].pct_change()
            df['vix_ma_7d'] = df['VIX'].rolling(24*7).mean()
        
        if 'DXY' in df.columns:
            df['dxy_change'] = df['DXY'].pct_change()
            df['btc_dxy_correlation'] = df['returns'].rolling(24*30).corr(df['dxy_change'])
        
        if 'GOLD' in df.columns:
            df['gold_returns'] = df['GOLD'].pct_change()
            df['btc_gold_correlation'] = df['returns'].rolling(24*30).corr(df['gold_returns'])
        
        new_features = df.shape[1] - initial_cols
        print(f"   Created {new_features} derived features")
        
        return df
    
    def create_target_labels(self, df):
        """Creates target labels for ML models."""
        
        print("\n[Target Labels] Creating prediction targets...")
        
        # Binary classification: Up or Down
        df['target_direction_1h'] = (df['future_return_1h'] > 0).astype(int)
        
        # Multi-class: Strong down, down, neutral, up, strong up
        df['target_multiclass_1h'] = pd.cut(
            df['future_return_1h'],
            bins=[-np.inf, -0.01, -0.002, 0.002, 0.01, np.inf],
            labels=[0, 1, 2, 3, 4]
        )
        
        # Regression target
        df['target_return_1h'] = df['future_return_1h']
        
        print("   ‚úÖ Created classification and regression targets")
        return df
    
    def clean_and_finalize(self, df):
        """
        OPTIMIZED: Final cleaning with MINIMAL data loss.
        
        Strategy:
        1. Only drop rows where CRITICAL features are missing
        2. For non-critical features, use smart imputation
        3. Remove lookback period NaNs (unavoidable)
        """
        
        print("\n[Finalization] Cleaning dataset...")
        
        # Replace infinite values with NaN
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        
        initial_rows = len(df)
        
        # Step 1: Drop rows with missing CRITICAL features only
        print(f"   Checking critical features: {self.critical_features}")
        df.dropna(subset=self.critical_features, inplace=True)
        critical_dropped = initial_rows - len(df)
        
        # Step 2: For remaining NaNs, use smart filling
        # Technical indicators: forward then backward fill
        tech_cols = [col for col in df.columns if any(x in col.lower() for x in 
                    ['sma', 'ema', 'rsi', 'macd', 'bb', 'atr', 'stoch', 'adx', 
                     'cci', 'willr', 'mfi', 'obv', 'vwap', 'momentum', 'volatility'])]
        
        if tech_cols:
            df[tech_cols] = df[tech_cols].ffill().bfill()
        
        # Step 3: Drop rows with NaN in target labels (can't train without labels)
        target_cols = [col for col in df.columns if 'target' in col.lower()]
        if target_cols:
            df.dropna(subset=target_cols, inplace=True)
        
        # Step 4: Final cleanup - drop any remaining rows with NaNs
        # (Should be minimal now - mostly from lookback windows at start)
        df.dropna(inplace=True)
        
        final_rows = len(df)
        total_dropped = initial_rows - final_rows
        retention_rate = (final_rows / initial_rows) * 100
        
        print(f"\n   üìä Data Retention Analysis:")
        print(f"   Initial rows: {initial_rows:,}")
        print(f"   Dropped (critical features): {critical_dropped:,}")
        print(f"   Dropped (targets/lookback): {total_dropped - critical_dropped:,}")
        print(f"   Final rows: {final_rows:,}")
        print(f"   ‚úÖ RETENTION RATE: {retention_rate:.1f}%")
        print(f"   Final shape: {df.shape}")
        
        return df
    
    def generate_all_features(self):
        """Main pipeline - OPTIMIZED VERSION."""
        
        if not self.load_all_data():
            return None
        
        print("\n" + "="*60)
        print("OPTIMIZED FEATURE ENGINEERING PIPELINE")
        print("="*60)
        
        # Start with OHLCV
        df = self.data['ohlcv'].copy()
        initial_ohlcv_rows = len(df)
        
        # Generate price-based features
        df = self.generate_price_features(df)
        
        # Merge external data sources (if available)
        if 'blockchain' in self.data:
            df = self.merge_external_data(df, 'blockchain', self.data['blockchain'])
        
        if 'sentiment' in self.data:
            df = self.merge_external_data(df, 'sentiment', self.data['sentiment'])
        
        if 'macro' in self.data:
            df = self.merge_external_data(df, 'macro', self.data['macro'])
        
        # Add derived features
        df = self.add_derived_features(df)
        
        # Create target labels
        df = self.create_target_labels(df)
        
        # Final cleaning (OPTIMIZED)
        df = self.clean_and_finalize(df)
        
        # Final summary
        print("\n" + "="*60)
        print("‚úÖ FEATURE ENGINEERING COMPLETE")
        print("="*60)
        print(f"Original OHLCV rows: {initial_ohlcv_rows:,}")
        print(f"Final feature rows: {len(df):,}")
        print(f"Overall retention: {len(df)/initial_ohlcv_rows*100:.1f}%")
        print(f"Total features: {df.shape[1]}")
        
        return df
    
    def save_features(self, df):
        """Saves the complete feature set."""
        
        if df is None:
            return
        
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        df.to_csv(FEATURE_DATA_PATH)
        
        print(f"\nüíæ Saved to: {FEATURE_DATA_PATH}")
        print(f"Date range: {df.index.min()} to {df.index.max()}")
        print(f"Duration: {(df.index.max() - df.index.min()).days} days")
        
        # Feature category summary
        tech = sum(1 for c in df.columns if any(x in c.lower() for x in ['sma', 'ema', 'rsi', 'macd']))
        blockchain = sum(1 for c in df.columns if any(x in c.lower() for x in ['hash', 'difficulty', 'tx_', 'nvt']))
        sentiment = sum(1 for c in df.columns if 'fear' in c.lower() or 'greed' in c.lower())
        macro = sum(1 for c in df.columns if any(x in c.lower() for x in ['sp500', 'vix', 'dxy', 'gold']))
        targets = sum(1 for c in df.columns if 'target' in c.lower())
        
        print(f"\nüìä Feature Categories:")
        print(f"   Technical: {tech}")
        print(f"   Blockchain: {blockchain}")
        print(f"   Sentiment: {sentiment}")
        print(f"   Macro: {macro}")
        print(f"   Targets: {targets}")

def main():
    """Main execution."""
    engine = EnhancedFeatureEngine()
    features_df = engine.generate_all_features()
    
    if features_df is not None:
        engine.save_features(features_df)
        
        # Display sample
        print("\n" + "="*60)
        print("SAMPLE DATA (last 5 rows)")
        print("="*60)
        display_cols = ['Close', 'RSI_14', 'MACD_12_26_9', 'fear_greed_value', 
                        'SP500', 'target_direction_1h']
        available = [c for c in display_cols if c in features_df.columns]
        if available:
            print(features_df[available].tail())

if __name__ == '__main__':
    main()